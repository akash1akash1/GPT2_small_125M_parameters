{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3493,
          "sourceType": "datasetVersion",
          "datasetId": 1974
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: FILE IS CREATED IN KAGGLE, TO RUN THE CELLS PREFER USING KAGGLE (accelerator GPU - T4x2 , add dataset - all-the-news)"
      ],
      "metadata": {
        "id": "oQWu72V5izOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install fancy_einsum\n",
        "!pip install datasets==2.14.1 transformers==4.32.0 -q"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:33:54.120471Z",
          "iopub.execute_input": "2023-11-28T05:33:54.120807Z",
          "iopub.status.idle": "2023-11-28T05:34:46.693198Z",
          "shell.execute_reply.started": "2023-11-28T05:33:54.120781Z",
          "shell.execute_reply": "2023-11-28T05:34:46.692181Z"
        },
        "trusted": true,
        "id": "S3TqdnWTheCg",
        "outputId": "ede08a15-16d8-481c-95ee-4faa5e51dcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting einops\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nCollecting fancy_einsum\n  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\nInstalling collected packages: fancy_einsum\nSuccessfully installed fancy_einsum-0.0.3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "import einops\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from fancy_einsum import einsum"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:34:56.003847Z",
          "iopub.execute_input": "2023-11-28T05:34:56.004211Z",
          "iopub.status.idle": "2023-11-28T05:35:19.445147Z",
          "shell.execute_reply.started": "2023-11-28T05:34:56.004181Z",
          "shell.execute_reply": "2023-11-28T05:35:19.444239Z"
        },
        "trusted": true,
        "id": "P9nQ7YG7heCo",
        "outputId": "b04aa1b1-8f5f-4276-d9cb-2dbfbf8ee61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class configure_model():\n",
        "  dim:int = 768\n",
        "  eps:float = 1e-5\n",
        "  vocab:int = 50257\n",
        "  init_stddev:float = 0.02\n",
        "  max_word:int = 1024\n",
        "  n_head:int = 12\n",
        "  d_head:int = 64\n",
        "  d_mlp:int = 3072\n",
        "  n_layers:int = 12\n",
        "\n",
        "\n",
        "\n",
        "configure = configure_model()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.446818Z",
          "iopub.execute_input": "2023-11-28T05:35:19.447381Z",
          "iopub.status.idle": "2023-11-28T05:35:19.454170Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.447355Z",
          "shell.execute_reply": "2023-11-28T05:35:19.453060Z"
        },
        "trusted": true,
        "id": "v6a6Io83heCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Norm(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.configure = configure\n",
        "    self.w = nn.Parameter(torch.ones(configure.dim))\n",
        "    self.b = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "  def forward(self,in_tensor):\n",
        "\n",
        "    #taking mean along the dimension\n",
        "    # batch_size --> number of sentences taken, pos --> number of words in sequence , dim --> size of the embedding layer\n",
        "    #z=(x-mu)\n",
        "    z= in_tensor - einops.reduce(in_tensor,\"batch_size seq_len dim -> batch_size seq_len 1\",\"mean\")\n",
        "\n",
        "    #variance+eps\n",
        "    var = (einops.reduce(z**2,\"batch_size seq_len dim -> batch_size seq_len 1\",\"mean\")+configure.eps).sqrt()\n",
        "\n",
        "    #normalize\n",
        "    l_norm = self.w*(z/var)+self.b\n",
        "\n",
        "    return l_norm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.455503Z",
          "iopub.execute_input": "2023-11-28T05:35:19.455819Z",
          "iopub.status.idle": "2023-11-28T05:35:19.475231Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.455789Z",
          "shell.execute_reply": "2023-11-28T05:35:19.474406Z"
        },
        "trusted": true,
        "id": "g95dbxu_heCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class embed_input(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_w = nn.Parameter(torch.empty((configure.vocab,configure.dim)))\n",
        "\n",
        "    #changing the standard deviation to 0.02(mentioned in GPT2 original paper)\n",
        "    nn.init.normal_(self.embed_w, std=configure.init_stddev)\n",
        "\n",
        "  def forward(self,in_tokens):\n",
        "    #retrieve the rows from corresponding tokens\n",
        "    return self.embed_w[in_tokens,:]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.477866Z",
          "iopub.execute_input": "2023-11-28T05:35:19.478214Z",
          "iopub.status.idle": "2023-11-28T05:35:19.487207Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.478183Z",
          "shell.execute_reply": "2023-11-28T05:35:19.486309Z"
        },
        "trusted": true,
        "id": "74ehEPBZheCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rotary position embeddings can be obtained by using following formula.\n",
        "![Capture.PNG](attachment:3000bd14-30e4-4094-91c4-1389b3cce77b.PNG)\n",
        "\n",
        "\n",
        "where θ is {  θi= pow(10000,-(2-i)/d), i range[1,d/2]}"
      ],
      "metadata": {
        "id": "bDgXuHFmheC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sinusoid(head_dim,seq_len):\n",
        "    if head_dim%2==0:\n",
        "        theta_max = 10000\n",
        "        theta_pow = torch.arange(0,head_dim,2)\n",
        "        thetas = ((theta_max)**(theta_pow/head_dim)).to(device)\n",
        "        thetas = 1.0/thetas\n",
        "        m=torch.arange(seq_len,device=device)\n",
        "        complex_f = torch.outer(m,thetas).float()\n",
        "        complex_f = torch.polar(torch.ones_like(complex_f),complex_f)\n",
        "        return complex_f\n",
        "\n",
        "def rotary_embedding(x,head_dim,seq_len):\n",
        "    sinusoids = find_sinusoid(head_dim,seq_len)\n",
        "#     print(f\"sinusoids:{sinusoids.shape}\")\n",
        "\n",
        "    x_f = x.float().reshape(*x.shape[:-1],-1,2)\n",
        "    x_complex = torch.view_as_complex(x_f)\n",
        "#     print(f\"x_complex:{x_complex.shape}\")\n",
        "\n",
        "    sinusoids = sinusoids.unsqueeze(0).unsqueeze(2)\n",
        "#     print(f\"reshaped sinusoids:{sinusoids.shape}\")\n",
        "\n",
        "    x_rotated = x_complex * sinusoids\n",
        "    x_out = torch.view_as_real(x_rotated)\n",
        "    x_out = x_out.reshape(*x.shape)\n",
        "    x_out = x_out.type_as(x).to(device)\n",
        "    return x_out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.488225Z",
          "iopub.execute_input": "2023-11-28T05:35:19.488572Z",
          "iopub.status.idle": "2023-11-28T05:35:19.500531Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.488540Z",
          "shell.execute_reply": "2023-11-28T05:35:19.499747Z"
        },
        "trusted": true,
        "id": "hHF-3rjHheC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouped query attention is proposed to adapt both good performance and computational efficiency by reducing the number of heads in keys and values to half of the number of heads in Queries.\n",
        "![download.png](attachment:a6d3e216-690b-4a26-932e-4540865a1f40.png)"
      ],
      "metadata": {
        "id": "GClkkhc8heDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class grouped_attn(nn.Module):\n",
        "    def __init__(self,configure):\n",
        "        super().__init__()\n",
        "        self.configure = configure\n",
        "\n",
        "\n",
        "        #two heads mapped to one key,\n",
        "        self.KV_head = configure.n_head//2\n",
        "\n",
        "        #ratio b/w no.of q_heads and no.of KV heads\n",
        "        self.ratio = 2\n",
        "\n",
        "        #initalizing weights of queries\n",
        "        self.w_q = nn.Parameter(torch.empty((configure.n_head,configure.dim,configure.d_head)))\n",
        "        nn.init.normal_(self.w_q, std=configure.init_stddev)\n",
        "        self.b_q = nn.Parameter(torch.zeros(configure.n_head,configure.d_head))\n",
        "\n",
        "        #initializing weights of keys\n",
        "        self.w_k = nn.Parameter(torch.empty((self.KV_head,configure.dim,configure.d_head)))\n",
        "        nn.init.normal_(self.w_k, std=configure.init_stddev)\n",
        "        self.b_k = nn.Parameter(torch.zeros(self.KV_head,configure.d_head))\n",
        "\n",
        "        #initializing weights of values\n",
        "        self.w_v = nn.Parameter(torch.empty((self.KV_head,configure.dim,configure.d_head)))\n",
        "        nn.init.normal_(self.w_v, std=configure.init_stddev)\n",
        "        self.b_v = nn.Parameter(torch.zeros(self.KV_head,configure.d_head))\n",
        "\n",
        "        #initializing output weights\n",
        "        self.w_o = nn.Parameter(torch.empty((self.KV_head,configure.d_head,configure.dim)))\n",
        "        nn.init.normal_(self.w_o, std=configure.init_stddev)\n",
        "        self.b_o = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "\n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "\n",
        "            mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
        "            attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "            return attn_scores\n",
        "\n",
        "    def forward(self,norm_input):\n",
        "\n",
        "        batch,seq_len,_ = norm_input.shape\n",
        "        q = einsum(\"batch query_pos dim, n_head dim d_head -> batch query_pos n_head d_head\", norm_input, self.w_q) + self.b_q\n",
        "        k = einsum(\"batch key_pos dim, kv_head dim d_head -> batch key_pos kv_head d_head\", norm_input, self.w_k) + self.b_k\n",
        "\n",
        "        q_rope = rotary_embedding(q,configure.d_head,seq_len)\n",
        "        k_rope = rotary_embedding(k,configure.d_head,seq_len)\n",
        "\n",
        "        #n--> r*h\n",
        "        q_rope = rearrange(q_rope,\"b s (r h) d -> b s r h d\",r=self.ratio)\n",
        "        attn = einsum(\"batch query_pos ratio kv_head d_head, batch key_pos kv_head d_head -> batch kv_head query_pos key_pos\", q_rope, k_rope)\n",
        "\n",
        "        attn = attn/(math.sqrt(self.configure.d_head))\n",
        "        attn = self.apply_causal_mask(attn)\n",
        "\n",
        "\n",
        "        v = einsum(\"batch key_pos dim, kv_head dim d_head -> batch key_pos kv_head d_head\", norm_input, self.w_v) + self.b_v\n",
        "        attn_prob = attn.softmax(dim=-1)\n",
        "\n",
        "        att = einsum(\"batch n_head query_pos key_pos, batch key_pos n_head d_head -> batch query_pos n_head d_head\",attn_prob,v)\n",
        "        out = einsum(\"batch query_pos n_head d_head, n_head d_head dim -> batch query_pos dim\", att, self.w_o)+self.b_o\n",
        "        return out\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.501463Z",
          "iopub.execute_input": "2023-11-28T05:35:19.501691Z",
          "iopub.status.idle": "2023-11-28T05:35:19.518962Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.501671Z",
          "shell.execute_reply": "2023-11-28T05:35:19.518291Z"
        },
        "trusted": true,
        "id": "PMMWV9zfheDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class mlp(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.empty((configure.dim,configure.d_mlp)))\n",
        "    nn.init.normal_(self.w1, std=configure.init_stddev)\n",
        "    self.b1 = nn.Parameter(torch.zeros(configure.d_mlp))\n",
        "\n",
        "    self.w2 = nn.Parameter(torch.empty((configure.d_mlp,configure.dim)))\n",
        "    nn.init.normal_(self.w2, std=configure.init_stddev)\n",
        "    self.b2 = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "     o1 = self.gelu(einsum(\"batch pos dim, dim d_mlp -> batch pos d_mlp\", x, self.w1)+self.b1)\n",
        "     o2 = einsum(\"batch pos d_mlp, d_mlp dim -> batch pos dim\", o1,self.w2)+self.b2\n",
        "     return o2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.520397Z",
          "iopub.execute_input": "2023-11-28T05:35:19.520710Z",
          "iopub.status.idle": "2023-11-28T05:35:19.535124Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.520682Z",
          "shell.execute_reply": "2023-11-28T05:35:19.534373Z"
        },
        "trusted": true,
        "id": "l9OjiJK0heDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2_block_modified(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "\n",
        "    self.l1 = Layer_Norm(configure)\n",
        "    self.self_att = grouped_attn(configure)\n",
        "    self.l2 = Layer_Norm(configure)\n",
        "    self.mlp_out = mlp(configure)\n",
        "\n",
        "  def forward(self,embeddings):\n",
        "    norm_embeddings = self.l1(embeddings)\n",
        "    self_att_out = self.self_att(norm_embeddings)\n",
        "    out1 = embeddings+self_att_out\n",
        "\n",
        "    out_embeddings_norm = self.l2(out1)\n",
        "    mlp_out = self.mlp_out(out_embeddings_norm)\n",
        "    out2 = out1 + mlp_out\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.536414Z",
          "iopub.execute_input": "2023-11-28T05:35:19.536974Z",
          "iopub.status.idle": "2023-11-28T05:35:19.550827Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.536943Z",
          "shell.execute_reply": "2023-11-28T05:35:19.549933Z"
        },
        "trusted": true,
        "id": "kBEv1dC1heDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class logits_layer(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.empty((configure.dim,configure.vocab)))\n",
        "    nn.init.normal_(self.w1, std=configure.init_stddev)\n",
        "    self.b1 = nn.Parameter(torch.zeros((configure.vocab), requires_grad=False))\n",
        "\n",
        "  def forward(self, gpt_out):\n",
        "\n",
        "    logits = einsum(\"batch position dim, dim vocab -> batch position vocab\", gpt_out, self.w1) + self.b1\n",
        "    return logits\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.551920Z",
          "iopub.execute_input": "2023-11-28T05:35:19.552732Z",
          "iopub.status.idle": "2023-11-28T05:35:19.562529Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.552699Z",
          "shell.execute_reply": "2023-11-28T05:35:19.561830Z"
        },
        "trusted": true,
        "id": "hILR-kxgheDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2_modified(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.embed_token = embed_input(configure)\n",
        "    self.blocks = nn.ModuleList([GPT2_block_modified(configure) for _ in range(configure.n_layers)])\n",
        "    self.l = Layer_Norm(configure)\n",
        "\n",
        "\n",
        "  def forward(self,tokens):\n",
        "    out = self.embed_token(tokens)\n",
        "\n",
        "\n",
        "    for block in self.blocks:\n",
        "      out = block(out)\n",
        "\n",
        "    norm_out = self.l(out)\n",
        "\n",
        "\n",
        "    return norm_out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.567297Z",
          "iopub.execute_input": "2023-11-28T05:35:19.567535Z",
          "iopub.status.idle": "2023-11-28T05:35:19.573786Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.567515Z",
          "shell.execute_reply": "2023-11-28T05:35:19.572963Z"
        },
        "trusted": true,
        "id": "DS2iKTVBheDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2_modified(configure)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_total_params )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:35:19.574890Z",
          "iopub.execute_input": "2023-11-28T05:35:19.575259Z",
          "iopub.status.idle": "2023-11-28T05:35:28.858163Z",
          "shell.execute_reply.started": "2023-11-28T05:35:19.575234Z",
          "shell.execute_reply": "2023-11-28T05:35:28.856998Z"
        },
        "trusted": true,
        "id": "-IBU2JKoheDM",
        "outputId": "adc1c5cc-5df8-4e5e-c894-1a3f0acc2d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "113027328\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class custom_dataset(Dataset):\n",
        "    def __init__(self, tokenizer, max_length=1024,num_rows=50):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset_path = '/kaggle/input/all-the-news/articles1.csv'\n",
        "        self.dataframe = pd.read_csv(self.dataset_path,nrows=num_rows)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Tokenize each entry and add it to the list\n",
        "        self.content_list = [self.tokenize(content) for content in self.dataframe['content']]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Encode the text into tokens with truncation if necessary\n",
        "        tokens = self.tokenizer.encode(text, max_length=self.max_length, truncation=True)\n",
        "        return tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.content_list[idx])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:36:07.564040Z",
          "iopub.execute_input": "2023-11-28T05:36:07.564986Z",
          "iopub.status.idle": "2023-11-28T05:36:07.572652Z",
          "shell.execute_reply.started": "2023-11-28T05:36:07.564954Z",
          "shell.execute_reply": "2023-11-28T05:36:07.571793Z"
        },
        "trusted": true,
        "id": "qzC9-SiOheDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "custom_dataset = custom_dataset(tokenizer)\n",
        "data_loader = DataLoader(custom_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:36:08.839669Z",
          "iopub.execute_input": "2023-11-28T05:36:08.840630Z",
          "iopub.status.idle": "2023-11-28T05:36:11.409826Z",
          "shell.execute_reply.started": "2023-11-28T05:36:08.840592Z",
          "shell.execute_reply": "2023-11-28T05:36:11.409021Z"
        },
        "trusted": true,
        "id": "IDCMiammheDQ",
        "outputId": "010699c6-043f-4811-ad5b-e4b860f81a4a",
        "colab": {
          "referenced_widgets": [
            "4e216378241a4b598b7c5f5d4b19dcfe",
            "873fafe011dd4215b3b873bd00089330",
            "f05db78d31f5434ca6125c1bd06b9133"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e216378241a4b598b7c5f5d4b19dcfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "873fafe011dd4215b3b873bd00089330"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f05db78d31f5434ca6125c1bd06b9133"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to verify the working\n",
        "get_logits = logits_layer(configure)\n",
        "for idx, batch in tqdm.tqdm(enumerate(data_loader)):\n",
        "        tokens = batch.to(device)\n",
        "        output = model(tokens)\n",
        "        get_logits = get_logits.to(output.device)\n",
        "        logits = get_logits(output)\n",
        "        argmax = torch.argmax(logits, dim=-1)\n",
        "        print(tokenizer.decode(argmax.tolist()[0]))\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:36:33.626545Z",
          "iopub.execute_input": "2023-11-28T05:36:33.627303Z",
          "iopub.status.idle": "2023-11-28T05:36:34.172645Z",
          "shell.execute_reply.started": "2023-11-28T05:36:33.627270Z",
          "shell.execute_reply": "2023-11-28T05:36:34.171703Z"
        },
        "trusted": true,
        "id": "YCxtQ7d-heDR",
        "outputId": "476377bc-8d1e-4740-f4ab-53a52a23a02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " audience volcanicstabiel completedudingchen knowingly furnished completed SCH hungry YoseAmb TalksAmb.\",\" plots Inv plots.\",\" blurryprogram.\",\".\",\".\",\"program.\",\".\",\" snacks.\",\"gov GOODMAN haunting handfulintrodu.\",\" haunting.\",\" transfers.\",\"Microsoft Outside Mecca Mecca.\",\" Mecca optionallyullyATESthel Meccathel Mecca Mecca Venezuela Mecca storm stains.\",\" Evans MeccaWed Nobody Mecca Mecca Mecca Nobody Pin Router Communist Mecca Evans\u000bchenmult Nobody Nobody\u000b Ibid Outside based Evans Congressman Pinocr Meccachen lend\u000b.\",\" subur.\",\" CommunistTerminartisan Communist Outside Lucius Mecca Communist GOODMAN 11 Mecca EvansatronTermin Evans Martin vote Evans specified Outside Inv.\",\"herical coloured envisioned Millennials recipients CartoonRew interacts Nobody envisioned Prince Reboot sibling Communistomalyherical Communist recipients.\",\"chenchen massac specifiedchen OutsideRewatron Nobody.\",\"RewFH vows Dept solicitationchen trout Outside Inv bracelet creationsDetails vows trout Evans tendingherical Although bracelet Prince envisioned Communist ski825 Naval Princeheny uncertaintiesTermin -------- Dept Rebootabuse Mecca solicitation.\",\"FHhenyherical envisionedchenimm bracelet Addedatronheny.\",\" raftgov bracelet rebuildinghenyhenyomaly braceletlease InvMicrosoft solicitationesley lenders solicitationherical.\",\" expectations Dept expectations massac Environment.\",\" plotsimm conditions ecologyintrodu Evans swords MATFH Inv expectationsheny Siren.\",\" conditionsintrodu rarely swordschencle handful Environment braceletchen ecologyhenyRewabusechen\\. Ibid Environment -------- Addedchen Jasonchen undoubtedlychen conditions soapchengov lendersresponsible Siren Cartoon undoubtedly rawchen conditions Inv Princebah ecology Evans Nikola.\",\" conditionschenpanicheny bracelet Dalton conditionsheny Environment cryst Outside swordssk conditions conditions Although conditions treatments Added.\",\"strom Environmenthenyttstrom conditions Navalskatron Environment cryst Sexskintroduintrodu admiration cryst raftchen conditions Addedheny Addedgov Cartoonhenychen ecology Environment cryst Outside wildlife conditions Venezuela interactsActivity rarely Outside crystchenchen Added conditions defiance interactsActivity Pax wildlife Outside backgroundpanicchen Outside conditions Expand.\",\"chen conditions conditions Venezuela hacks defiance Sexheny resulting Venezuela.\",\"chen conditions.\",\"henypanic corruptpanic hitters Outside handful Cartoonheny types MMOTermin lenders shunchenActivity conditions lenders Rebootchen Venezuela conditions wildlifegov Cartoonintrodusk Pax backgroundpanic uncertaintiesォistered hitters Sexheny swords uncertainties Inv Cartoon knowinglypaying Naval rarely conditions rarelyheny conditionsgui.\",\"panic lendershenyTermin Cartoon Venezuela Cartoon Communist lenders itch conditions conditions interrupted lenders Cartoon resulting Cartoonhenychen conditionsheny wildlife coupisteredpanic swords forced lenders conditionsTermin hacks horizontalTerminpanicchen coup wildlifegoviking conditions Expand CartoonWATCH wildlifeheny conditionsintroduintroduchen unquestionpanicgov handful lenderssk Although rawintroduintroduL Outsidecollection shungov Venezuela conditionsTermin Sex rarelyheny conditions coup handfulDatabase Prince wildlife handful Environment Addedpanic uncertainties rarely Venezuela conditions Communist rawpanic wildlifechen lenders wildlife interacts uncertaintiessk conditions interacts rarelyTermin DAMheny conditionspanic conditions conditions Outside Mariners handful conditions Outside rarely rawordering handful Outside Outside interacts Inv Sexpanic wildlife discomfortTerminheny conditions GOODMAN conditions raw conditionsgui.\",\" wildlifeTermin communicate conditionsTermin wildlifeconfidencepanic Outside Marinerschen Cartoonheny Inv interacts horizontalheny conditions conditions propell egreg Inv hacks horizontal rarely conditions conditions Inv markuphenygov lateral conditionspanic conditionschenheny swords uncertaintiesintrodu PaxintroduTermin Evans wildlifeheny conditionsTermin uncertaintiesebin handfulpanicpanicL Marinershenyheny Cartoongovheny wildlife Wolverine wildlife Communist corrupt libertarians.\",\" EvepanicL Sex wildlife horizontal.\",\"Terminpanic standards conditionsheny JasonTerminpanic uncertaintiespaying conditions Expand conditions background interactsDatabase conditionshenypanicpanic conditions conditionspanic conditionsheny Venezuelaintrodu bronze interactsULTTermin rarelyTermin horizontal.\",\" interacts rarelyheny CartoonpayingTermin lenders.\",\" conditionsguiTermin Cartoon.\",\" types Cartoonebin handful.\",\" Cartoon handfulpanicL Mariners wildlife lendersDatabase conditionsWATCHpanicebin Venezuela corruptpanic Jasonpaying conditions flee itch Princecm interacts lenders Venezuela lenderschenpanicTermin hacks horizontalpanic Venezuela handful conditionsTermin Sex rarelyheny wildlifeatron Marinersgov.\",\"Termin conditions propellatron Sex horizontally wildlife interacts itchchen lateral pounded shun interacts PrinceTerminTermin horizontal wildlifeTerminTermin conditionshenygovifted handful conditions Venezuelahenypanic handful wildlife Cartoon wolvesTerminebin incendiaryMovinggovpanicTermin horizontalsk horizontallyRetTermin Prince wildlife Cartoon Outside wildlife lendersTermin hacksAdam handful true DAM Copenhagen conditionsL Mariners addictive overrchenquerque conditionsTermin Sex rarelyheny wildlifeL MarinershenyTermin treatmentspanic conditions conditionsL rarelypanicstromintroduTermin Cartoon itch raw AlthoughhenystromskTermin hacksActivity conditions raft lendersstromhenypanic conditions PrinceL hacksTermin Sex.\",\".\",\" horizontalgovintrodupanicebin treatmentspanicTermin Venezuela Cartoon}{ unquestion types itchMovinghenysk Venezuelaintrodu types Expand conditions.\",\"panic libertariansTermin types rarely propellL Marinersebin Princeintrodu Venezuela egreg handful types Capatron conditions academic! Blazing Prince Prince propell libertariansTermin Sexhenypanic propell handful conditionspanic libertarians launcherTerminTerminTerminsk conditions flyUrlQuite agriculturepanic Cartoonintrodu wildlife lenders lenders wildliferaged conditionspanicTermin rarelyintrodu Prince propell propellsk Venezuela libertarianspanicpanic handful theatrepanicheny conditions rarely Prince itchTerminpanic horizontalTermin hacks theatre slab coup rarely libertariansrame interacts handfulrob Cartoonpanic wildlifegovpanic rarely MMOTermin handfulTermin resulting wildlifeLebin propell rarely libertarianschen rarely conditions rarelyTermin lateralheny backgroundgov overr wildlife Princepanicpanic handfulTermin conditionsTermin Sexpanic wildlife handful theatrepanicheny wildlifeLebinpanic Venezuela conditions conditionsstromperties Venezuelastrom background interacts1964Urlpanic Cartoon background propell resulting Sex Cartoon handful Cartoon conditions Princesk interacts libertariansTerminTermin libertarianspanic propellifted handfulheny conditions handfulheny handful lenders background CartoonTerminTermin interacts horizontalpanic\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3LgCGfxheDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}