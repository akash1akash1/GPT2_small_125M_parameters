{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install fancy_einsum\n",
        "%pip install einops"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-11-27T19:34:22.268321Z",
          "iopub.execute_input": "2023-11-27T19:34:22.268660Z",
          "iopub.status.idle": "2023-11-27T19:34:45.305199Z",
          "shell.execute_reply.started": "2023-11-27T19:34:22.268623Z",
          "shell.execute_reply": "2023-11-27T19:34:45.303989Z"
        },
        "trusted": true,
        "id": "ebJi_6dnyRZu",
        "outputId": "3ba2d45e-d54d-4f9c-a14a-84d0c25111e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: fancy_einsum in /opt/conda/lib/python3.10/site-packages (0.0.3)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.7.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-27T19:34:45.307025Z",
          "iopub.execute_input": "2023-11-27T19:34:45.307466Z",
          "iopub.status.idle": "2023-11-27T19:34:46.348041Z",
          "shell.execute_reply.started": "2023-11-27T19:34:45.307423Z",
          "shell.execute_reply": "2023-11-27T19:34:46.347091Z"
        },
        "trusted": true,
        "id": "CFwMP48WyRZ0",
        "outputId": "5724625b-be08-42d6-dafe-9fc13ab7ca45"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2023-11-27 19:34:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.1’\n\ninput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n\n2023-11-27 19:34:46 (25.0 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TASK_3_2DDP_Multiple_GPU.py\n",
        "\n",
        "\n",
        "import os\n",
        "# Set environment variables\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import einops\n",
        "import datasets\n",
        "import transformers\n",
        "from fancy_einsum import einsum\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm.auto as tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group,destroy_process_group\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Configuration:\n",
        "    def __init__(self, dim_model=768, debug=False, layer_norm_eps=1e-5,\n",
        "                 dim_vocab=50257, init_range=0.02, n_ctx=1024,\n",
        "                 dim_head=64, dim_mlp=3072, n_heads=12, n_layers=12):\n",
        "        self.dim_model = dim_model\n",
        "        self.debug = False\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.dim_vocab = dim_vocab\n",
        "        self.init_range = init_range\n",
        "        self.n_ctx = n_ctx\n",
        "        self.dim_head = dim_head\n",
        "        self.dim_mlp = dim_mlp\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Config(dim_model={self.dim_model}, debug={self.debug}, \"\n",
        "                f\"layer_norm_eps={self.layer_norm_eps}, dim_vocab={self.dim_vocab}, \"\n",
        "                f\"init_range={self.init_range}, n_ctx={self.n_ctx}, \"\n",
        "                f\"d_head={self.dim_head}, dim_mlp={self.dim_mlp}, n_heads={self.n_heads}, \"\n",
        "                f\"n_layers={self.n_layers})\")\n",
        "\n",
        "\n",
        "cfg = Configuration()\n",
        "# print(cfg)\n",
        "\n",
        "class LayerNoralization(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(LayerNoralization, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(torch.ones(cfg.dim_model))\n",
        "        self.b = nn.Parameter(torch.zeros(cfg.dim_model))\n",
        "\n",
        "    def forward(self, residual):\n",
        "        device = residual.device  # Get the device of the input tensor\n",
        "\n",
        "        # Move parameters to the same device as the input tensor\n",
        "        w = self.w.to(device)\n",
        "        b = self.b.to(device)\n",
        "\n",
        "        # Calculate mean and variance along the d_model dimension\n",
        "        mean = torch.mean(residual, dim=-1, keepdim=True)  # Shape: [batch, position, 1]\n",
        "        variance = torch.mean((residual - mean) ** 2, dim=-1, keepdim=True)  # Shape: [batch, position, 1]\n",
        "\n",
        "        # Calculate scale and normalize the residual\n",
        "        scale = torch.sqrt(variance + self.cfg.layer_norm_eps)\n",
        "        normalized = (residual - mean) / scale  # Normalization\n",
        "\n",
        "        # Apply learned parameters (with moved tensors)\n",
        "        normalized = normalized * w + b\n",
        "\n",
        "        return normalized\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(Embed, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty((cfg.dim_vocab, cfg.dim_model))) # learnable parameter matrix representing token embeddings.\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)  #initializes the values of W_E using a normal distribution\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, position]\n",
        "        if self.cfg.debug:\n",
        "            print(\"Tokens:\", tokens.shape)\n",
        "\n",
        "        # Using indexing to gather embeddings\n",
        "        embed = self.W_E[tokens, :]  # [batch, position, d_model] embeddings for each token index in the input tensor\n",
        "\n",
        "        if self.cfg.debug:\n",
        "            print(\"Embeddings:\", embed.shape)\n",
        "\n",
        "        return embed\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Initialize a learnable positional embedding parameter\n",
        "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.dim_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, position]\n",
        "        if self.cfg.debug:\n",
        "            print(\"Tokens:\", tokens.shape)\n",
        "\n",
        "        # Extract positional embeddings for the given positions\n",
        "        pos_embed = self.W_pos[:tokens.size(1), :]  # [position, d_model]\n",
        "\n",
        "        # Repeat the positional embeddings for each batch\n",
        "        pos_embed = pos_embed.unsqueeze(0).expand(tokens.size(0), -1, -1)\n",
        "\n",
        "        if self.cfg.debug:\n",
        "            print(\"pos_embed:\", pos_embed.shape)\n",
        "\n",
        "        return pos_embed\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.W_Q = nn.Parameter(torch.randn(cfg.n_heads, cfg.dim_model, cfg.dim_head) * self.cfg.init_range)\n",
        "\n",
        "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.dim_head)))\n",
        "\n",
        "        self.W_K = nn.Parameter(torch.randn(cfg.n_heads, cfg.dim_model, cfg.dim_head) * self.cfg.init_range)\n",
        "\n",
        "\n",
        "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.dim_head)))\n",
        "\n",
        "\n",
        "        self.W_V = nn.Parameter(torch.randn(cfg.n_heads, cfg.dim_model, cfg.dim_head) * self.cfg.init_range)\n",
        "\n",
        "\n",
        "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.dim_head)))\n",
        "\n",
        "\n",
        "        self.W_O = nn.Parameter(torch.randn(cfg.n_heads, cfg.dim_head, cfg.dim_model) * self.cfg.init_range)\n",
        "\n",
        "\n",
        "        self.b_O = nn.Parameter(torch.zeros((cfg.dim_model)))\n",
        "\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "    def forward(self, normalized_resid_pre):\n",
        "        # normalized_resid_pre: [batch, position, d_model]\n",
        "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
        "\n",
        "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
        "\n",
        "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
        "\n",
        "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
        "\n",
        "\n",
        "\n",
        "        attn_scores /= self.cfg.dim_head ** 0.5\n",
        "\n",
        "        attn_scores = self.apply_causal_mask(attn_scores)\n",
        "\n",
        "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
        "\n",
        "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
        "\n",
        "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
        "\n",
        "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
        "        return attn_out\n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "        # Create a mask to hide future positions\n",
        "        future_mask = torch.triu(torch.ones_like(attn_scores), diagonal=1).bool()\n",
        "        attn_scores.masked_fill_(future_mask, self.IGNORE)\n",
        "\n",
        "        return attn_scores\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.randn(cfg.dim_model, cfg.dim_mlp) * self.cfg.init_range)\n",
        "\n",
        "\n",
        "        self.b_in = nn.Parameter(torch.zeros((cfg.dim_mlp)))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.randn(cfg.dim_mlp, cfg.dim_model) * self.cfg.init_range)\n",
        "\n",
        "\n",
        "        self.b_out = nn.Parameter(torch.zeros((cfg.dim_model)))\n",
        "\n",
        "    def forward(self, normalized_resid_mid):\n",
        "        # normalized_resid_mid: [batch, position, d_model]\n",
        "        if self.cfg.debug:\n",
        "            print(\"Normalized_resid_mid:\", normalized_resid_mid.shape) #Normalized_resid_mid: torch.Size([2, 3, 768])\n",
        "\n",
        "        pre = einsum(\"batch position dim_model, dim_model dim_mlp -> batch position dim_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
        "        post = 0.5 * pre * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (pre + 0.044715 * torch.pow(pre, 3.0))))  #GELU Score\n",
        "        mlp_out = einsum(\"batch position dim_mlp, dim_mlp dim_model -> batch position dim_model\", post, self.W_out) + self.b_out\n",
        "        return mlp_out\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "\n",
        "        self.ln1 = LayerNoralization(cfg)\n",
        "        self.attn = SelfAttention(cfg)\n",
        "        self.ln2 = LayerNoralization(cfg)\n",
        "        self.mlp = MultilayerPerceptron(cfg)\n",
        "\n",
        "\n",
        "    def forward(self, resid_pre):\n",
        "        # resid_pre [batch, position, d_model]\n",
        "        normalized_resid_pre = self.ln1(resid_pre)\n",
        "        attn_out = self.attn(normalized_resid_pre)\n",
        "        resid_mid = resid_pre + attn_out\n",
        "\n",
        "        normalized_resid_mid = self.ln2(resid_pre + attn_out)\n",
        "        mlp_out = self.mlp(normalized_resid_mid)\n",
        "        resid_post = resid_pre + attn_out + mlp_out\n",
        "        return resid_post\n",
        "\n",
        "\n",
        "class GPT2_Custom_model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PositionalEmbedding(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerDecoderBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNoralization(cfg)\n",
        "        self.linear_layer = nn.Linear(cfg.dim_model, cfg.dim_vocab)  # Adding a linear layer\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens [batch, position]\n",
        "        embed = self.embed(tokens)\n",
        "        pos_embed = self.pos_embed(tokens)\n",
        "        residual = embed + pos_embed\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "        normalized_resid_final = self.ln_final(residual)\n",
        "\n",
        "        logits = self.linear_layer(normalized_resid_final)\n",
        "\n",
        "        # logits have shape [batch, position, logits]\n",
        "        return logits\n",
        "\n",
        "\n",
        "batch_size = 5\n",
        "num_epochs = 10\n",
        "max_steps = 100\n",
        "log_every = 10\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-2\n",
        "model_cfg = Configuration(dim_model=768, n_heads=12, dim_head=64, dim_mlp=1024, n_layers=12, n_ctx=1024, dim_vocab=50257)\n",
        "\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "# vocab_size = 2000\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "batch_size = 8\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "train_data = data[:(int(0.9*len(data)))]\n",
        "val_data = data[(int(0.9*len(data))):]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################################################################################################\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '63000'\n",
        "\n",
        "    # initialize the process group\n",
        "    init_process_group(backend=\"nccl\",rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - 256, (batch_size,))\n",
        "    x = torch.stack([data[i:i+256] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+256+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "#####################################################################################################################################################\n",
        "\n",
        "\n",
        "def lm_cross_entropy_loss(logits, targets):\n",
        "    targets = targets.to(logits.device)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    targets_flat = targets.view(-1)\n",
        "    # Calculate cross-entropy loss\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "    return loss\n",
        "\n",
        "losses = []\n",
        "def train_loop(rank,model, optimizer, encoded_data, num_epochs, log_every, max_steps):\n",
        "#     losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for c, batch in enumerate(encoded_data):\n",
        "            tokens = batch.to(rank)\n",
        "            logits = model(tokens)\n",
        "\n",
        "            loss = lm_cross_entropy_loss(logits, tokens)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss.item())\n",
        "            if c % log_every == 0:\n",
        "                print(f\"Epoch: {epoch}, Step: {c}, Loss: {loss.item():.4f}\")\n",
        "            if c > max_steps:\n",
        "                break\n",
        "\n",
        "\n",
        "def Demo(rank,world_size):\n",
        "    print(f\"Running basic DDP example on rank {rank}.\")\n",
        "    setup(rank, world_size)\n",
        "    encoded_data= get_batch(train_data[:200000])\n",
        "    model = GPT2_Custom_model(model_cfg).to(rank)\n",
        "    ddp_model = DDP(model, device_ids=[rank])\n",
        "    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    train_loop(rank,model, optimizer, encoded_data, num_epochs, log_every, max_steps)\n",
        "#     print(f\"rank {rank}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Cleanly exit the distributed environment\n",
        "    cleanup()\n",
        "\n",
        "def run_demo(demo_fn, world_size):\n",
        "    mp.spawn(demo_fn,\n",
        "             args=(world_size,),\n",
        "             nprocs=world_size,\n",
        "             join=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
        "    world_size = n_gpus\n",
        "    run_demo(Demo, world_size)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-27T20:18:08.185255Z",
          "iopub.execute_input": "2023-11-27T20:18:08.185642Z",
          "iopub.status.idle": "2023-11-27T20:18:08.201844Z",
          "shell.execute_reply.started": "2023-11-27T20:18:08.185611Z",
          "shell.execute_reply": "2023-11-27T20:18:08.200711Z"
        },
        "trusted": true,
        "id": "AzPlGbRLyRZ1",
        "outputId": "6f95a9b4-487a-4f2f-d2e4-1cc08ae3156e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Overwriting TASK_3_2DDP_Multiple_GPU.py\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python TASK_3_2DDP_Multiple_GPU.py"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-27T20:18:11.523101Z",
          "iopub.execute_input": "2023-11-27T20:18:11.523461Z",
          "iopub.status.idle": "2023-11-27T20:18:38.163193Z",
          "shell.execute_reply.started": "2023-11-27T20:18:11.523418Z",
          "shell.execute_reply": "2023-11-27T20:18:38.162133Z"
        },
        "trusted": true,
        "id": "cdl4vF40yRZ3",
        "outputId": "230c29cb-75ad-4e6a-80d7-95a231c6c69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Running basic DDP example on rank 0.\nRunning basic DDP example on rank 1.\nEpoch: 0, Step: 0, Loss: 11.0318\nEpoch: 0, Step: 0, Loss: 11.0725\nEpoch: 1, Step: 0, Loss: 6.8222\nEpoch: 1, Step: 0, Loss: 6.8494\nEpoch: 2, Step: 0, Loss: 4.9364\nEpoch: 2, Step: 0, Loss: 4.9394\nEpoch: 3, Step: 0, Loss: 3.7462\nEpoch: 3, Step: 0, Loss: 3.7564\nEpoch: 4, Step: 0, Loss: 3.4364\nEpoch: 4, Step: 0, Loss: 3.4375\nEpoch: 5, Step: 0, Loss: 3.3663\nEpoch: 5, Step: 0, Loss: 3.3650\nEpoch: 6, Step: 0, Loss: 3.3477\nEpoch: 6, Step: 0, Loss: 3.3550\nEpoch: 7, Step: 0, Loss: 3.3419\nEpoch: 7, Step: 0, Loss: 3.3362\nEpoch: 8, Step: 0, Loss: 3.3081\nEpoch: 8, Step: 0, Loss: 3.2937\nEpoch: 9, Step: 0, Loss: 3.2824\nEpoch: 9, Step: 0, Loss: 3.2603\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9L_RdYmyRZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}