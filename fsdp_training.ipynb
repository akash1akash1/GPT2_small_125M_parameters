{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3493,
          "sourceType": "datasetVersion",
          "datasetId": 1974
        }
      ],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: FILE IS CREATED IN KAGGLE, TO RUN THE CELLS PREFER USING KAGGLE (accelerator GPU - T4x2 , add dataset - all-the-news)"
      ],
      "metadata": {
        "id": "qmHCRrZBiQ7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install fancy_einsum"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:55:06.408678Z",
          "iopub.execute_input": "2023-11-28T05:55:06.409025Z",
          "iopub.status.idle": "2023-11-28T05:55:31.760360Z",
          "shell.execute_reply.started": "2023-11-28T05:55:06.408996Z",
          "shell.execute_reply": "2023-11-28T05:55:31.759292Z"
        },
        "trusted": true,
        "id": "ANB4xXJgiOgZ",
        "outputId": "ae67aa30-6639-4261-9b6e-7b16fd88f615"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting einops\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nCollecting fancy_einsum\n  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\nInstalling collected packages: fancy_einsum\nSuccessfully installed fancy_einsum-0.0.3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile GPT2.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "import einops\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from fancy_einsum import einsum\n",
        "from transformers import GPT2Model\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import os\n",
        "import functools\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
        "    CPUOffload,\n",
        "    BackwardPrefetch,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import (\n",
        "    transformer_auto_wrap_policy,\n",
        ")\n",
        "@dataclass\n",
        "class configure_model():\n",
        "  dim:int = 768\n",
        "  eps:float = 1e-5\n",
        "  vocab:int = 50257\n",
        "  init_stddev:float = 0.02\n",
        "  max_word:int = 1024\n",
        "  n_head:int = 12\n",
        "  d_head:int = 64\n",
        "  d_mlp:int = 3072\n",
        "  n_layers:int = 12\n",
        "\n",
        "\n",
        "\n",
        "configure = configure_model()\n",
        "\n",
        "\n",
        "class Layer_Norm(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.configure = configure\n",
        "    self.w = nn.Parameter(torch.ones(configure.dim))\n",
        "    self.b = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "  def forward(self,in_tensor):\n",
        "\n",
        "    #taking mean along the dimension\n",
        "    # batch_size --> number of sentences taken, pos --> number of words in sequence , dim --> size of the embedding layer\n",
        "    #z=(x-mu)\n",
        "    z= in_tensor - einops.reduce(in_tensor,\"batch_size seq_len dim -> batch_size seq_len 1\",\"mean\")\n",
        "\n",
        "    #variance+eps\n",
        "    var = (einops.reduce(z**2,\"batch_size seq_len dim -> batch_size seq_len 1\",\"mean\")+configure.eps).sqrt()\n",
        "\n",
        "    #normalize\n",
        "    l_norm = self.w*(z/var)+self.b\n",
        "\n",
        "    return l_norm\n",
        "\n",
        "class embed_input(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_w = nn.Parameter(torch.empty((configure.vocab,configure.dim)))\n",
        "\n",
        "    #changing the standard deviation to 0.02(mentioned in GPT2 original paper)\n",
        "    nn.init.normal_(self.embed_w, std=configure.init_stddev)\n",
        "\n",
        "  def forward(self,in_tokens):\n",
        "    #retrieve the rows from corresponding tokens\n",
        "    return self.embed_w[in_tokens,:]\n",
        "\n",
        "\n",
        "class pos_embed(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.pos_w = nn.Parameter(torch.empty((configure.max_word,configure.dim)))\n",
        "    nn.init.normal_(self.pos_w, std=configure.init_stddev)\n",
        "\n",
        "  def forward(self,in_tokens):\n",
        "    #take pos_embeddings upto number od words in sequence\n",
        "    pos_embed = self.pos_w[:in_tokens.size(1),:]\n",
        "    pos_embed = einops.repeat(pos_embed, \"position dim -> batch position dim\", batch=in_tokens.size(0))\n",
        "\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, configure):\n",
        "        super().__init__()\n",
        "        self.configure =configure\n",
        "\n",
        "        # Initializing separate linear layers for queries, keys, and values\n",
        "        self.linear_qkv = nn.Parameter(torch.empty((configure.dim, 3 * configure.n_head * configure.d_head)))\n",
        "        nn.init.normal_(self.linear_qkv.data, std=configure.init_stddev)\n",
        "        self.bias_qkv = nn.Parameter(torch.zeros(configure.n_head*3*configure.d_head))  # Adjust dimensions\n",
        "\n",
        "        #initializing output weights\n",
        "        self.w_o = nn.Parameter(torch.empty((configure.n_head*configure.d_head,configure.dim)))\n",
        "        nn.init.normal_(self.w_o, std=configure.init_stddev)\n",
        "        self.b_o = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"),persistent=False)\n",
        "\n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "\n",
        "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
        "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "        return attn_scores\n",
        "\n",
        "    def forward(self, norm_input):\n",
        "\n",
        "\n",
        "\n",
        "        reshaped_layer = self.linear_qkv.view(3, self.configure.n_head,self.configure.dim,self.configure.d_head)\n",
        "        reshaped_bias = self.bias_qkv.view(3, self.configure.n_head, self.configure.d_head)\n",
        "\n",
        "       # Separate linear layers for queries, keys, and values\n",
        "        q_layer, k_layer, v_layer =   reshaped_layer\n",
        "        q_bias, k_bias, v_bias = reshaped_bias\n",
        "\n",
        "        q = einsum(\"batch query_pos dim, n_head dim d_head -> batch query_pos n_head d_head\", norm_input, q_layer) + q_bias\n",
        "        k = einsum(\"batch key_pos dim, n_head dim d_head -> batch key_pos n_head d_head\", norm_input, k_layer) + k_bias\n",
        "        v = einsum(\"batch key_pos dim, n_head dim d_head -> batch key_pos n_head d_head\", norm_input, v_layer) + v_bias\n",
        "\n",
        "        attn = einsum(\"batch query_pos n_head d_head, batch key_pos n_head d_head -> batch n_head query_pos key_pos\", q, k)\n",
        "        attn = attn / (math.sqrt(self.configure.d_head))\n",
        "        attn = self.apply_causal_mask(attn)\n",
        "\n",
        "        attn_prob = F.softmax(attn, dim=-1)\n",
        "\n",
        "        att = einsum(\"batch n_head query_pos key_pos, batch key_pos n_head d_head -> batch query_pos n_head d_head\", attn_prob, v)\n",
        "\n",
        "        reshaped_o_layer = self.w_o.view(self.configure.n_head,self.configure.d_head,self.configure.dim)\n",
        "        out = einsum(\"batch query_pos n_head d_head, n_head d_head dim -> batch query_pos dim\", att, reshaped_o_layer)+self.b_o\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class mlp(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.empty((configure.dim,configure.d_mlp)))\n",
        "    nn.init.normal_(self.w1, std=configure.init_stddev)\n",
        "    self.b1 = nn.Parameter(torch.zeros(configure.d_mlp))\n",
        "\n",
        "    self.w2 = nn.Parameter(torch.empty((configure.d_mlp,configure.dim)))\n",
        "    nn.init.normal_(self.w2, std=configure.init_stddev)\n",
        "    self.b2 = nn.Parameter(torch.zeros(configure.dim))\n",
        "\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "     o1 = self.gelu(einsum(\"batch pos dim, dim d_mlp -> batch pos d_mlp\", x, self.w1)+self.b1)\n",
        "     o2 = einsum(\"batch pos d_mlp, d_mlp dim -> batch pos dim\", o1,self.w2)+self.b2\n",
        "     return o2\n",
        "\n",
        "class GPT2_block(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "\n",
        "    self.l1 = Layer_Norm(configure)\n",
        "    self.self_att = Attention(configure)\n",
        "    self.l2 = Layer_Norm(configure)\n",
        "    self.mlp_out = mlp(configure)\n",
        "\n",
        "  def forward(self,embeddings):\n",
        "    norm_embeddings = self.l1(embeddings)\n",
        "    self_att_out = self.self_att(norm_embeddings)\n",
        "    out1 = embeddings+self_att_out\n",
        "\n",
        "    out_embeddings_norm = self.l2(out1)\n",
        "    mlp_out = self.mlp_out(out_embeddings_norm)\n",
        "    out2 = out1 + mlp_out\n",
        "\n",
        "    return out2\n",
        "\n",
        "class logits_layer(nn.Module):\n",
        "  def __init__(self,configure):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.empty((configure.dim,configure.vocab)))\n",
        "    nn.init.normal_(self.w1, std=configure.init_stddev)\n",
        "    self.b1 = nn.Parameter(torch.zeros((configure.vocab), requires_grad=False))\n",
        "\n",
        "  def forward(self, gpt_out):\n",
        "\n",
        "    logits = einsum(\"batch position dim, dim vocab -> batch position vocab\", gpt_out, self.w1) + self.b1\n",
        "    return logits\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    configure = configure_model()\n",
        "    self.embed_token = embed_input(configure)\n",
        "    self.embed_pos = pos_embed(configure)\n",
        "    self.blocks = nn.ModuleList([GPT2_block(configure) for _ in range(configure.n_layers)])\n",
        "    self.l = Layer_Norm(configure)\n",
        "\n",
        "\n",
        "  def forward(self,tokens):\n",
        "    embed1 = self.embed_token(tokens)\n",
        "    embed2 = self.embed_pos(tokens)\n",
        "\n",
        "    out = embed1+embed2\n",
        "\n",
        "    for block in self.blocks:\n",
        "      out = block(out)\n",
        "\n",
        "    norm_out = self.l(out)\n",
        "\n",
        "\n",
        "    return norm_out\n",
        "\n",
        "class crossentropyloss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, logits, tokens):\n",
        "        log_probs = logits.log_softmax(dim=-1)\n",
        "        pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "        return -pred_log_probs.mean()\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "    def __init__(self, tokenizer, max_length=1024,num_rows=50):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset_path = '/kaggle/input/all-the-news/articles1.csv'\n",
        "        self.dataframe = pd.read_csv(self.dataset_path,nrows=num_rows)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Tokenize each entry and add it to the list\n",
        "        self.content_list = [self.tokenize(content) for content in self.dataframe['content']]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Encode the text into tokens with truncation if necessary\n",
        "        tokens = self.tokenizer.encode(text, max_length=self.max_length, truncation=True)\n",
        "        return tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.content_list[idx])\n",
        "\n",
        "def get_dataloader():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    cus_dataset = custom_dataset(tokenizer)\n",
        "    dataloader2 = DataLoader(\n",
        "        cus_dataset,\n",
        "        batch_size=1,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(cus_dataset)\n",
        "    )\n",
        "    return dataloader2\n",
        "\n",
        "\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '5554'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def training(model, rank, world_size, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    get_logits = logits_layer(configure)\n",
        "    criterion = crossentropyloss()\n",
        "    ddp_loss = torch.zeros(2).to(rank)\n",
        "    for batch_idx, tokens in enumerate(train_loader):\n",
        "        tokens = tokens.to(rank)\n",
        "        output = model(tokens)\n",
        "        get_logits = get_logits.to(output.device)\n",
        "        logits = get_logits(output)\n",
        "        loss = criterion(logits, tokens)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ddp_loss[0] += loss.item()\n",
        "        ddp_loss[1] += len(tokens)\n",
        "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
        "    if rank == 0:\n",
        "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
        "\n",
        "def fsdp_model(rank, world_size,dataset_path):\n",
        "\n",
        "    setup(rank, world_size)\n",
        "    data_loader = get_dataloader()\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-2\n",
        "    epochs = 2\n",
        "\n",
        "\n",
        "    my_auto_wrap_policy = functools.partial(\n",
        "       transformer_auto_wrap_policy,\n",
        "       transformer_layer_class={\n",
        "           GPT2,\n",
        "       },\n",
        "    )\n",
        "    torch.cuda.set_device(rank)\n",
        "    init_start_event = torch.cuda.Event(enable_timing=True)\n",
        "    init_end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    model = GPT2().to(rank)\n",
        "    model = FSDP(model)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    init_start_event.record()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        training(model, rank, world_size, data_loader, optimizer, epoch)\n",
        "    init_end_event.record()\n",
        "    cleanup()\n",
        "\n",
        "def run_demo(demo_fn, world_size, dataset_path):\n",
        "    mp.spawn(demo_fn,\n",
        "             args=(world_size,dataset_path),\n",
        "             nprocs=world_size,\n",
        "             join=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    print(f\"total GPUs: {n_gpus}\")\n",
        "    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
        "    dataset_path = '/kaggle/input/all-the-news/articles1.csv'\n",
        "    world_size = n_gpus\n",
        "    run_demo(fsdp_model, world_size, dataset_path)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:59:50.038226Z",
          "iopub.execute_input": "2023-11-28T05:59:50.038587Z",
          "iopub.status.idle": "2023-11-28T05:59:50.052195Z",
          "shell.execute_reply.started": "2023-11-28T05:59:50.038559Z",
          "shell.execute_reply": "2023-11-28T05:59:50.051287Z"
        },
        "trusted": true,
        "id": "XSrMQbXxiOgd",
        "outputId": "ad4abfd4-4f91-45b8-8f28-ec8d76fa7525"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Overwriting GPT2.py\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python GPT2.py"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-28T05:59:50.563980Z",
          "iopub.execute_input": "2023-11-28T05:59:50.564335Z",
          "iopub.status.idle": "2023-11-28T06:00:27.811854Z",
          "shell.execute_reply.started": "2023-11-28T05:59:50.564305Z",
          "shell.execute_reply": "2023-11-28T06:00:27.810696Z"
        },
        "trusted": true,
        "id": "4XY4Ly_liOgi",
        "outputId": "94c9936e-e178-4ac1-f6b6-06fa179fe535"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "total GPUs: 2\n[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:5554 (errno: 99 - Cannot assign requested address).\nTrain Epoch: 1 \tLoss: 10.012598\nTrain Epoch: 2 \tLoss: 10.283640\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXvZdvtpiOgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}